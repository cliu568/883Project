{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 32, 32, 16]\n",
      "[None, 32, 32, 16]\n",
      "[None, 32, 32, 16]\n",
      "[None, 32, 32, 16]\n",
      "[None, 32, 32, 16]\n",
      "[None, 16, 16, 32]\n",
      "[None, 16, 16, 32]\n",
      "[None, 16, 16, 32]\n",
      "[None, 16, 16, 32]\n",
      "[None, 8, 8, 64]\n",
      "[None, 8, 8, 64]\n",
      "[None, 8, 8, 64]\n",
      "[None, 8, 8, 64]\n",
      "[None, 64]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import google.protobuf\n",
    "\n",
    "import numpy as np\n",
    "from data_utils import *\n",
    "from batch_norm import *\n",
    "from summary import _activation_summary,_add_loss_summaries\n",
    "\n",
    "batch_size = 200\n",
    "initfact=10\n",
    "learning_rate=.1\n",
    "path='dataset'\n",
    "path+='/cifar-100-python'\n",
    "n_epochs = 800\n",
    "NUM_CLASSES=20\n",
    "valid_set=1000\n",
    "time_per_epoch=10\n",
    "repeat_layer=2\n",
    "visual=True\n",
    "ifbatchnorm=True\n",
    "weight_d=0\n",
    "ifDrop=False\n",
    "def elapsed():\n",
    "    return (time.time()-t)/60\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "    with tf.device('/cpu:0'):\n",
    "        var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    var = _variable_on_cpu(name, shape,\n",
    "                         tf.truncated_normal_initializer(stddev=stddev))\n",
    "    if wd is not None and wd !=0:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "def eval(xx,yy):\n",
    "    return sess.run(accuracy,\n",
    "          feed_dict={\n",
    "              x:xx,\n",
    "              y:yy,\n",
    "              is_training: False})\n",
    "\n",
    "def svd_orthonormal(shape):\n",
    "    flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "    a = np.random.standard_normal(flat_shape)\n",
    "    u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "    q = u if u.shape == flat_shape else v\n",
    "    q = q.reshape(shape)\n",
    "    return q\n",
    "tf.reset_default_graph()\n",
    "######################architecture##########################################\n",
    "trainWs=[]\n",
    "x = tf.placeholder(tf.float32, [None, 32,32,3])\n",
    "y = tf.placeholder(tf.float32, [None])\n",
    "teacher_label=tf.placeholder(tf.float32, [None,20])\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "LUSV=tf.placeholder(tf.float32)\n",
    "lr=tf.placeholder(tf.float32)\n",
    "\n",
    "kernel = _variable_with_weight_decay('conv0',\n",
    "                                 shape=[3, 3, 3, 16],\n",
    "                                 stddev=np.sqrt(2.0/3/9)\n",
    "                                 , wd=weight_d)\n",
    "trainWs.append(kernel)\n",
    "orthoInit0=kernel.assign(svd_orthonormal(kernel.get_shape().as_list()))\n",
    "upd0=kernel.assign(kernel/LUSV)\n",
    "ConvLayer0 = tf.nn.conv2d(x, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "print(ConvLayer0.get_shape().as_list())\n",
    "if ifbatchnorm:\n",
    "    net = batch_norm(ConvLayer0,is_training,scope='conv0')\n",
    "net = tf.nn.relu(net)\n",
    "if visual:\n",
    "    _activation_summary(net)\n",
    "    \n",
    "resLayer=[]\n",
    "orthoInit=[]\n",
    "kernel_upd=[]\n",
    "for block in range(3):\n",
    "    blocks=[]\n",
    "    nfilters=16<<block\n",
    "    for layer in range(repeat_layer):\n",
    "        layers=[]\n",
    "        net_copy=net\n",
    "        for i in range(2):\n",
    "            name = 'block_%d/layer_%d/conv%d' % (block, layer,i)\n",
    "            if layer==0 and i==0 and block!=0 :\n",
    "                up=2\n",
    "            else:\n",
    "                up=1\n",
    "            kernel = _variable_with_weight_decay(name,\n",
    "                    shape=[3, 3,\n",
    "                           net.get_shape().as_list()[3],\n",
    "                           nfilters],\n",
    "                    stddev=np.sqrt(2.0/9/nfilters),\n",
    "                    wd=weight_d)\n",
    "            trainWs.append(kernel)\n",
    "            orthoInit.append(kernel.assign(svd_orthonormal(kernel.get_shape().as_list())))\n",
    "            kernel_upd.append(kernel.assign(kernel/LUSV))\n",
    "            \n",
    "            net  = tf.nn.conv2d(net,\n",
    "                    kernel,\n",
    "                    [1,up,up, 1],\n",
    "                    padding='SAME')\n",
    "            \n",
    "            print(net.get_shape().as_list())\n",
    "            layers.append(net)\n",
    "            if ifbatchnorm:\n",
    "                net = batch_norm(net,is_training, scope=name)\n",
    "            #net = BatchNorm(net)\n",
    "            net = tf.nn.relu(net)\n",
    "            if ifDrop:\n",
    "                net = tf.nn.dropout(net,.5)\n",
    "            if visual:\n",
    "                _activation_summary(net)\n",
    "        blocks.append(layers)\n",
    "\n",
    "        # residual function (identity shortcut)\n",
    "        if net_copy.get_shape().as_list()[1]!=net.get_shape().as_list()[1]:\n",
    "            net_copy=tf.nn.avg_pool(net_copy,[1,2,2,1],\n",
    "                    strides=[1,2,2,1],padding='VALID')\n",
    "            net_copy=tf.pad(net_copy,[[0,0],[0,0],[0,0],[0,int(nfilters/2)]])\n",
    "        net = net + net_copy\n",
    "    resLayer.append(blocks)\n",
    "\n",
    "#Global avg pooling\n",
    "net_shape = net.get_shape().as_list()\n",
    "net = tf.nn.avg_pool(net,\n",
    "              ksize=[1, net_shape[1], net_shape[2], 1],\n",
    "              strides=[1, 1, 1, 1], \n",
    "              padding='VALID',name='global_pooling')\n",
    "net_shape = net.get_shape().as_list()\n",
    "hid=net_shape[1] * net_shape[2] * net_shape[3]\n",
    "net = tf.reshape(net,\n",
    "        [-1, hid])\n",
    "if ifDrop:\n",
    "    net = tf.nn.dropout(net,.5)\n",
    "print(net.get_shape().as_list())\n",
    "#softmax\n",
    "weights = _variable_with_weight_decay('softmax_w',\n",
    "    [hid, NUM_CLASSES],\n",
    "    stddev=np.sqrt(2.0/hid/NUM_CLASSES),\n",
    "    wd=weight_d)\n",
    "biases = _variable_on_cpu('softmax_b',\n",
    "    [NUM_CLASSES],\n",
    "    tf.constant_initializer(0.0))\n",
    "\n",
    "trainWs.append(weights)\n",
    "trainWs.append(biases)\n",
    "\n",
    "weights_orth=weights.assign(svd_orthonormal(weights.get_shape().as_list()))\n",
    "weights_upd=weights.assign(weights/LUSV)\n",
    "\n",
    "softmax_linear = tf.add(tf.matmul(net, weights), biases, name='softmax')\n",
    "\n",
    "y = tf.cast(y, tf.int64)\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      softmax_linear, y, name='cross_entropy_per_example')\n",
    "if visual:\n",
    "    _activation_summary(cross_entropy)\n",
    "    summary_op = tf.merge_all_summaries()\n",
    "\n",
    "cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "tf.add_to_collection('losses', cross_entropy_mean)\n",
    "cross_entropy_mean=tf.add_n(tf.get_collection('losses'))\n",
    "# train\n",
    "#train_step=tf.train.MomentumOptimizer(lr,.9).minimize(cross_entropy_mean)\n",
    "\n",
    "cost = tf.square(teacher_label - softmax_linear)\n",
    "teaching=tf.train.AdagradOptimizer(lr).minimize(cost)\n",
    "train_step=tf.train.AdagradOptimizer(lr).minimize(cross_entropy_mean)\n",
    "\n",
    "#predict\n",
    "correct_prediction=tf.equal(tf.argmax(softmax_linear,1),y)\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,'float'))\n",
    "tess=[cross_entropy_mean,train_step]\n",
    "\n",
    "########################training########################################\n",
    "# load data\n",
    "Xtr, Ytr, Xte, Yte=load_CIFAR100(path)\n",
    "# simple preprocessing\n",
    "mean_image = np.mean(Xtr, axis=0)\n",
    "Xtr -= mean_image\n",
    "Xte -= mean_image\n",
    "#img_var=Xtr.std(0)\n",
    "#Xtr/=img_var\n",
    "#Xte/=img_var\n",
    "#M=Xtr.mean(0)\n",
    "#[D,V]=np.linalg.eig(np.cov(Xtr,rowvar=0))\n",
    "#\n",
    "#P = V.dot(np.diag(np.sqrt(1/(D + 0.1)))).dot(V.T)\n",
    "#Xtr = Xtr.dot(P)\n",
    "#Xte=Xte.dot(P)\n",
    "\n",
    "def nextBatch():\n",
    "    idx=np.random.choice(numTrain,batch_size)\n",
    "    return Xtr[idx], Ytr[idx], idx\n",
    "\n",
    "numTrain=len(Xtr)-valid_set\n",
    "iter_per_epoch=numTrain // batch_size\n",
    "\n",
    "# %% We now create a new session to actually perform the initialization the\n",
    "# variables:\n",
    "saver = tf.train.Saver()\n",
    "sess=tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#saver.restore(sess,'summary/3_90.ckpt')\n",
    "if visual:\n",
    "    summary_writer = tf.train.SummaryWriter(\"./summary\", sess.graph)\n",
    "#########################LSUV#############################################3\n",
    "#max_iter = 20;\n",
    "#needed_variance =.1\n",
    "#margin = 0.02*needed_variance;\n",
    "#batch_xs,batch_ys=nextBatch()\n",
    "#bn=False\n",
    "#kernel_val,initWeights=sess.run([orthoInit0,ConvLayer0],\n",
    "#        feed_dict={x: batch_xs, y: batch_ys, is_training: bn})\n",
    "#for t in range(max_iter):\n",
    "#    variance=np.var(initWeights)\n",
    "#    print('var',variance)\n",
    "#    if abs(variance-needed_variance)<margin:\n",
    "#        break\n",
    "#    _,initWeights=sess.run([upd0,ConvLayer0],\n",
    "#            feed_dict={x: batch_xs, y: batch_ys, is_training: bn,LUSV:np.sqrt(variance/needed_variance)})\n",
    "#\n",
    "#for i in range(3):\n",
    "#    for j in range(repeat_layer):\n",
    "#        for k in range(2):\n",
    "#            print('LUSV init',i,j,k)\n",
    "#            kernel_val,initWeights=sess.run([orthoInit[i*repeat_layer*2+j*2+k],resLayer[i][j][k]],\n",
    "#                    feed_dict={x: batch_xs, y: batch_ys, is_training: bn})\n",
    "#            for t in range(max_iter):\n",
    "#                variance=np.var(initWeights)\n",
    "#                print(i,j,k,'var',variance)\n",
    "#                if abs(variance-needed_variance)<margin:\n",
    "#                    break\n",
    "#                kernel_val,initWeights=sess.run([kernel_upd[i*repeat_layer*2+j*2+k],resLayer[i][j][k]],\n",
    "#                        feed_dict={x: batch_xs, y: batch_ys, is_training: bn,LUSV:np.sqrt(variance/needed_variance)})\n",
    "#\n",
    "#kernel_val,initWeights=sess.run([weights_orth,cross_entropy],\n",
    "#        feed_dict={x: batch_xs, y: batch_ys, is_training: bn})\n",
    "#for t in range(max_iter):\n",
    "#    variance=np.var(initWeights)\n",
    "#    print('var',variance)\n",
    "#    if abs(variance-needed_variance)<margin:\n",
    "#        break\n",
    "#    _,initWeights=sess.run([weights_upd,cross_entropy],\n",
    "#            feed_dict={x: batch_xs, y: batch_ys, is_training: bn,LUSV:np.sqrt(variance/needed_variance)})\n",
    "########################################################################### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=time.time()\n",
    "losses=[]\n",
    "valacces=[]\n",
    "trainacces=[]\n",
    "best=0\n",
    "best_val=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 3.18085,time 0.0550193508466\n",
      "loss 2.91768,time 0.975338633855\n",
      "loss 2.82731,time 1.8051082015\n",
      "loss 2.94049,time 2.67100296815\n",
      "loss 2.74096,time 3.51496226788\n",
      "loss 2.70248,time 4.36299171845\n",
      "loss 2.6197,time 5.19010740121\n",
      "loss 2.57808,time 6.00378763278\n",
      "loss 2.68878,time 6.83465378284\n",
      "loss 2.61367,time 7.67170203527\n",
      "loss 2.65941,time 8.54515798489\n",
      "epoch2 avg_loss:2.7710588572 train acc:0.065 val acc:0.079\n",
      "Model saved in file: summary/2_2.ckpt\n",
      "loss 2.47063,time 8.86896824837\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-50a431485dab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch_i\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             loss,_=sess.run([cross_entropy_mean,teaching],\n\u001b[1;32m----> 8\u001b[1;33m                 feed_dict={x: batch_xs, y: batch_ys,teacher_label:teacher[idx], is_training: True,lr:learning_rate})\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             loss,_=sess.run([cross_entropy_mean,train_step],\n",
      "\u001b[1;32m/home/eli/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 343\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    344\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eli/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 578\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    579\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eli/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 651\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    652\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/eli/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    656\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/eli/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    640\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m--> 642\u001b[1;33m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch_i in range(2,n_epochs):\n",
    "    avg_loss=0\n",
    "    for batch_i in range(iter_per_epoch):\n",
    "        batch_xs,batch_ys,idx=nextBatch()   \n",
    "        \n",
    "        if epoch_i<10:\n",
    "            loss,_=sess.run([cross_entropy_mean,teaching],\n",
    "                feed_dict={x: batch_xs, y: batch_ys,teacher_label:teacher[idx], is_training: True,lr:learning_rate})\n",
    "        else:\n",
    "            loss,_=sess.run([cross_entropy_mean,train_step],\n",
    "                feed_dict={x: batch_xs, y: batch_ys, is_training: True,lr:learning_rate})\n",
    "        avg_loss+=loss\n",
    "        \n",
    "        if batch_i%int(iter_per_epoch/10)==0:\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if visual:\n",
    "                summary_str = sess.run(summary_op,\n",
    "                        feed_dict={x: batch_xs, y: batch_ys, is_training: False})\n",
    "                summary_writer.add_summary(summary_str, epoch_i*iter_per_epoch+batch_i)\n",
    "            print('loss '+str(loss)+',time '+str(elapsed()))\n",
    "    train_acc=eval( batch_xs,batch_ys )\n",
    "    val_acc=eval(Xte[:1000],Yte[:1000])\n",
    "    print(\"epoch\"+str(epoch_i)+\n",
    "            \" avg_loss:\"+str(avg_loss/iter_per_epoch)+\n",
    "            \" train acc:\"+ str(train_acc)+\n",
    "            \" val acc:\"+ str(val_acc))\n",
    "    \n",
    "    valacces.append(val_acc)\n",
    "    trainacces.append(train_acc)\n",
    "    if best_val<val_acc:\n",
    "        best_val=val_acc\n",
    "        best=epoch_i\n",
    "    save_path = saver.save(sess,'summary/'+str(repeat_layer)+'_'+ str(epoch_i)+\".ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "    if epoch_i>11:\n",
    "        learning_rate=.01\n",
    "    if epoch_i>16:\n",
    "        learning_rate=.005\n",
    "        if elapsed()>165:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summary/2_3.ckpt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "    \n",
    "saver.save(sess,'summary/'+str(repeat_layer)+'_'+ str(epoch_i)+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc:0.575\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess,'summary/'+str(repeat_layer)+'_'+str(best)+'.ckpt')\n",
    "print(\"test acc:\"+str(eval(Xte[:1000],Yte[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.restore(sess,'summary/2_1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get teacher labels \n",
    "\n",
    "saver.restore(sess,'summary/'+str(repeat_layer)+'_'+str(9)+'.ckpt')\n",
    "teacher=None\n",
    "for i in range(0,len(Xtr),200):\n",
    "    last=np.minimum(len(Xtr),i+200)\n",
    "    tmp=sess.run(softmax_linear,\n",
    "        feed_dict={x: Xtr[i:last], y: Ytr[i:last], is_training: False})\n",
    "    if teacher is None:\n",
    "        teacher=tmp\n",
    "    else:\n",
    "        teacher=np.concatenate((teacher,tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('teacher.pickle','w') as f:\n",
    "    pickle.dump(teacher,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "with open('teacher.pickle') as f:\n",
    "    teacher=pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
